{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- v5 is started on 6th December 2022, just after examinations\n",
    "- It is meant to recap on what has been done and try to implement Datagen\n",
    "- Without the use of Datagen, the server keeps crashing due to memory overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "\n",
    "import nrrd\n",
    "import random\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, Conv3DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Activation, MaxPool2D, Concatenate\n",
    "\n",
    "from ipywidgets import interact, interactive, IntSlider, ToggleButtons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.keras.__version__)\n",
    "print(tf.__version__)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices[3])\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[3], True)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/'\n",
    "\n",
    "z_train = os.path.join(data_dir, 'training_data_z')\n",
    "z_train_image = os.path.join(z_train, 'training_images/training_images')\n",
    "z_train_mask = os.path.join(z_train, 'training_masks/training_masks')\n",
    "\n",
    "z_valid = os.path.join(data_dir, 'valid_data_z')\n",
    "z_valid_image = os.path.join(z_valid, 'valid_images/valid_images')\n",
    "z_valid_mask = os.path.join(z_valid, 'valid_masks/valid_masks')\n",
    "\n",
    "z_test = os.path.join(data_dir, 'testing_data_z')\n",
    "z_test_image = os.path.join(z_test, 'testing_images/testing_images')\n",
    "z_test_mask = os.path.join(z_test, 'testing_masks/testing_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Helper Functions\n",
    "def read_nrrd_file(filepath):\n",
    "    '''read and load volume'''\n",
    "    pixelData, header = nrrd.read(filepath)\n",
    "    return pixelData[:,:,:96]\n",
    "\n",
    "def normalize(volume):\n",
    "    min = -1000 # min value of our data : -1000\n",
    "    max = 5000 # max value of our data : 5013\n",
    "    range = max - min\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / range\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "def process_scan(path):\n",
    "    volume = read_nrrd_file(path)\n",
    "    volume = normalize(volume)\n",
    "    return volume\n",
    "\n",
    "def sorted_alnum(l):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key : [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key = alphanum_key)\n",
    "\n",
    "def training_generator():\n",
    "    train_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/training_data_z/training_images/training_images'\n",
    "    train_mask_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/training_data_z/training_masks/training_masks'\n",
    "\n",
    "    train_data_with_96 = [15, 16, 20, 21, 23, 33, 37, 52, 64, 69, 72, 76, 77, 83, 92, 94, 95, 97, 103, 116, 123, 129, 157, 159, 162, 163, 170, 171, 180, 191, 192, 198, 200, 207, 209]\n",
    "    \n",
    "    train_path = sorted_alnum([os.path.join(train_dir, file) for file in os.listdir(train_dir)  if int(re.findall(r'\\d+', file)[0]) in train_data_with_96])\n",
    "    train_mask_path = sorted_alnum([os.path.join(train_mask_dir, file) for file in os.listdir(train_mask_dir)  if int(re.findall(r'\\d+', file)[0]) in train_data_with_96])\n",
    "    \n",
    "    for i in range(len(train_path)):\n",
    "        x = process_scan(train_path[i])\n",
    "        y = read_nrrd_file(train_mask_path[i])\n",
    "        \n",
    "        yield x,y\n",
    "        \n",
    "def validation_generator(valid_path, valid_mask):\n",
    "    \n",
    "    valid_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/valid_data_z/valid_images/valid_images'\n",
    "    valid_mask_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/valid_data_z/valid_masks/valid_masks'\n",
    "\n",
    "    valid_data_with_96 = [213, 219, 221, 227, 231, 238, 240, 243, 249, 250, 255, 256, 257, 270]\n",
    "\n",
    "    valid_path = sorted_alnum([os.path.join(valid_dir, file) for file in os.listdir(valid_dir)  if int(re.findall(r'\\d+', file)[0]) in valid_data_with_96])\n",
    "    valid_mask_path = sorted_alnum([os.path.join(valid_mask_dir, file) for file in os.listdir(valid_mask_dir)  if int(re.findall(r'\\d+', file)[0]) in valid_data_with_96])\n",
    "    \n",
    "    for i in range(len(valid_path)):\n",
    "        x = process_scan(valid_path[i])\n",
    "        y = read_nrrd_file(valid_mask_path[i])\n",
    "        yield x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = tf.data.Dataset.from_generator(training_generator,(tf.float32, tf.float32))\n",
    "validation_loader = tf.data.Dataset.from_generator(validation_generator,(tf.float32, tf.float32))\n",
    "\n",
    "ds = train_loader.batch(10)\n",
    "\n",
    "train_dataset = train_loader.shuffle(4)\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "train_dataset = train_dataset.batch(2, drop_remainder=True).prefetch(8)\n",
    "\n",
    "valid_dataset = validation_loader.shuffle(4)\n",
    "valid_dataset = valid_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "valid_dataset = valid_dataset.batch(2, drop_remainder=True).prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = Conv3D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)   #Not in the original network. \n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv3D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)  #Not in the original network\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPooling3D((2, 2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block\n",
    "#skip features gets input from encoder for concatenation\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv3DTranspose(num_filters, (2, 2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Build Unet using the blocks\n",
    "def build_unet(input_shape, n_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024) #Bridge\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    if n_classes == 1:  #Binary\n",
    "      activation = 'sigmoid'\n",
    "    else:\n",
    "      activation = 'softmax'\n",
    "\n",
    "    outputs = Conv3D(n_classes, 1, padding=\"same\", activation=activation)(d4)  #Change the activation based on n_classes\n",
    "    print(activation)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and coefficients to be used during training:\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    smoothing_factor = 1.0\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return (2. * K.sum(flat_y_true * flat_y_pred) + smoothing_factor) / (K.sum(flat_y_true) + K.sum(flat_y_pred) + smoothing_factor)\n",
    "\n",
    "def dice_coefficient_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    return 1.0 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "#Define parameters for our model.\n",
    "n_classes = 1\n",
    "patch_size = 32\n",
    "channels=3\n",
    "\n",
    "LR = 0.001\n",
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "\n",
    "\n",
    "model = build_unet((64,64,32,3), n_classes = 1)\n",
    "model.compile(optimizer = opt, loss=dice_coefficient_loss, metrics=dice_coefficient)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/home/tester/jianhoong/jh_fyp_work/3D_UNet/trials/3DUNet_ModelCSVLogs/UNet_Approach3_v5.csv'\n",
    "model_checkpoint_path = '/home/tester/jianhoong/jh_fyp_work/3D_UNet/ModelCheckpoints/Approach3_v5.hdf5'\n",
    "\n",
    "my_callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "    EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n",
    "    CSVLogger(csv_path, separator = ',', append = True),\n",
    "    ModelCheckpoint(filepath = model_checkpoint_path,\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    verbose = 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = train_dataset.take(1)\n",
    "# images, labels = list(data)[0]\n",
    "# images = images.numpy()\n",
    "# image = images[0]\n",
    "# print(\"Dimension of the CT scan is:\", image.shape)\n",
    "# plt.imshow(np.squeeze(image[:, :, 30]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.input_shape)\n",
    "# print(train_data.shape)\n",
    "print(model.output_shape)\n",
    "# print(train_mask.shape)\n",
    "print(\"-------------------\")\n",
    "# print(train_data.max())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "history = model.fit(\n",
    "        train_dataset, \n",
    "        validation_data = valid_dataset,\n",
    "        epochs=50,\n",
    "        shuffle = True,\n",
    "        verbose=2,\n",
    "        callbacks = my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the patients with 3mm thickness CT Scan and have mask labels within first 96 slices\n",
    "# These data are generated from \"ct_scans_data\" directory -> \"understanding_CTSCAN.ipynb\"\n",
    "# Take note that not all patients have up to 96 slices, thus the smaller cubes may not amount to 96 slice\n",
    "\n",
    "train_data = [4,5,9,10,15,16,18,19,20,21,22,23,28,29,33,34,35,37,44,51,52,58,64,68,69,72,73,76,77,79,82,83,90,91,92,93,94,95,97,103,104,116,123,126,129,131,134,136,139,142,149,151,155,157,159,162,163,164,169,170,171,179,180,186,188,191,192,198,200,201,206,207,208,209]\n",
    "valid_data = [213,214,215,217,219,220,221,224,226,227,228,231,237,238,240,243,248,249,250,254,255,256,257,264,266,270]\n",
    "test_data = [271,272,274,277,280,283,286,287,288,293,294,296,297,298,300,303,305,306,314,315,317,320,322,340,342,345,347]\n",
    "\n",
    "train_data_with_96 = [15, 16, 20, 21, 23, 33, 37, 52, 64, 69, 72, 76, 77, 83, 92, 94, 95, 97, 103, 116, 123, 129, 157, 159, 162, 163, 170, 171, 180, 191, 192, 198, 200, 207, 209]\n",
    "valid_data_with_96 = [213, 219, 221, 227, 231, 238, 240, 243, 249, 250, 255, 256, 257, 270]\n",
    "test_data_with_96 = [271, 272, 274, 277, 283, 286, 287, 293, 296, 300, 303, 305, 306, 314]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = sorted_alnum([os.path.join(z_train_image, file) for file in os.listdir(z_train_image)  if int(re.findall(r'\\d+', file)[0]) in train_data])\n",
    "# train_mask_path = sorted_alnum([os.path.join(z_train_mask, file) for file in os.listdir(z_train_mask)  if int(re.findall(r'\\d+', file)[0]) in train_data])\n",
    "\n",
    "# valid_path = sorted_alnum([os.path.join(z_valid_image, file) for file in os.listdir(z_valid_image)  if int(re.findall(r'\\d+', file)[0]) in valid_data])\n",
    "# valid_mask_path = sorted_alnum([os.path.join(z_valid_mask, file) for file in os.listdir(z_valid_mask)  if int(re.findall(r'\\d+', file)[0]) in valid_data])\n",
    "\n",
    "# test_path = sorted_alnum([os.path.join(z_test_image, file) for file in os.listdir(z_test_image)  if int(re.findall(r'\\d+', file)[0]) in test_data])\n",
    "# test_mask_path = sorted_alnum([os.path.join(z_test_mask, file) for file in os.listdir(z_test_mask)  if int(re.findall(r'\\d+', file)[0]) in test_data])\n",
    "\n",
    "train_path = sorted_alnum([os.path.join(z_train_image, file) for file in os.listdir(z_train_image)  if int(re.findall(r'\\d+', file)[0]) in train_data_with_96])\n",
    "train_mask_path = sorted_alnum([os.path.join(z_train_mask, file) for file in os.listdir(z_train_mask)  if int(re.findall(r'\\d+', file)[0]) in train_data_with_96])\n",
    "\n",
    "valid_path = sorted_alnum([os.path.join(z_valid_image, file) for file in os.listdir(z_valid_image)  if int(re.findall(r'\\d+', file)[0]) in valid_data_with_96])\n",
    "valid_mask_path = sorted_alnum([os.path.join(z_valid_mask, file) for file in os.listdir(z_valid_mask)  if int(re.findall(r'\\d+', file)[0]) in valid_data_with_96])\n",
    "\n",
    "test_path = sorted_alnum([os.path.join(z_test_image, file) for file in os.listdir(z_test_image)  if int(re.findall(r'\\d+', file)[0]) in test_data_with_96])\n",
    "test_mask_path = sorted_alnum([os.path.join(z_test_mask, file) for file in os.listdir(z_test_mask)  if int(re.findall(r'\\d+', file)[0]) in test_data_with_96])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(scan_paths, mask_paths):\n",
    "    \"\"\"\n",
    "    Processes all training ct scan cubes ~ 46000 cubes currently\n",
    "    \"\"\"\n",
    "    scan_store = np.zeros((1,64,64,32)) # Initialize 1 cube of size (32 x 32 x 32) full of zeroes as np.array cannot concatenate without same dimension\n",
    "    mask_store = np.zeros((1,64,64,32))\n",
    "\n",
    "    for i in range(len(scan_paths)):\n",
    "        scan_pixelData = process_scan(scan_paths[i])\n",
    "        mask_pixelData = read_nrrd_file(mask_paths[i])\n",
    "\n",
    "        scan_patch = patchify(scan_pixelData, (64,64,32), step = 16) # Yield 15 x 15 x 3 of size 64 x 64 x 32 cubes\n",
    "        mask_patch = patchify(mask_pixelData, (64,64,32), step = 16)\n",
    "\n",
    "        input_img = np.reshape(scan_patch, (-1, scan_patch.shape[3], scan_patch.shape[4], scan_patch.shape[5])) # Collapse 15 x 15 x 3 into 675 cubes x 64 x 64 x 32\n",
    "        input_mask = np.reshape(mask_patch, (-1, mask_patch.shape[3], mask_patch.shape[4], mask_patch.shape[5]))\n",
    "\n",
    "        scan_store = np.append(scan_store, input_img, axis = 0)\n",
    "        mask_store = np.append(mask_store, input_mask, axis = 0)\n",
    "\n",
    "        print(scan_store.shape)\n",
    "        print(mask_store.shape)\n",
    "    return scan_store[1:,:], mask_store[1:,:] # Don't return the first index which is the initialized 0 empty array\n",
    "\n",
    "\n",
    "def get_cubes(all_scan_cubes, all_mask_cubes, BACKBONE,desired_size = 300):\n",
    "    scan_storage = list()\n",
    "    mask_storage = list()\n",
    "\n",
    "    while len(scan_storage) < desired_size:\n",
    "        if len(scan_storage) > desired_size:\n",
    "            break\n",
    "        random_idx = random.randint(0, len(all_scan_cubes)-1)\n",
    "        if np.sum(all_mask_cubes[random_idx]) > 0:\n",
    "            scan_storage.append(all_scan_cubes[random_idx])\n",
    "            mask_storage.append(all_mask_cubes[random_idx])\n",
    "            print(f'cube {random_idx} stored', ' : ', f' Current samples: {len(scan_storage)}')\n",
    "\n",
    "    scan_storage = np.array(scan_storage)\n",
    "    mask_storage = np.array(mask_storage)\n",
    "\n",
    "    processed_scan = np.stack((scan_storage,)*3, axis=-1)\n",
    "    processed_mask = np.expand_dims(mask_storage, axis=4)\n",
    "\n",
    "    preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "    return preprocess_input(processed_scan), processed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scans, all_masks = process_all(train_path, train_mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(scan_paths, mask_paths,desired_size = 300):\n",
    "    scan_storage = list()\n",
    "    mask_storage = list()\n",
    "    patients_processed = list()\n",
    "\n",
    "    while len(scan_storage) < desired_size:\n",
    "        \n",
    "        random_idx = random.randint(0, len(scan_paths)-1)\n",
    "        patient_idx = int(re.findall(r'\\d+', scan_paths[random_idx][-14:-11])[0]) # Extracting patient number from path name\n",
    "        patients_processed.append(patient_idx) # Extract numerical patient index from path string\n",
    "\n",
    "        print(f'Processing Patient {patient_idx} data')\n",
    "        scan_pixelData = process_scan(scan_paths[random_idx]) # process_scan includes normalizing\n",
    "        mask_pixelData = read_nrrd_file(mask_paths[random_idx]) # Don't need to normalize mask\n",
    "\n",
    "        scan_patch = patchify(scan_pixelData, (64,64,32), step = 32) # Yield 15 x 15 x 3 of size 64 x 64 x 32 cubes\n",
    "        mask_patch = patchify(mask_pixelData, (64,64,32), step = 32)\n",
    "\n",
    "        input_img = np.reshape(scan_patch, (-1, scan_patch.shape[3], scan_patch.shape[4], scan_patch.shape[5])) # Collapse 15 x 15 x 3 into 675 cubes x 64 x 64 x 32\n",
    "        input_mask = np.reshape(mask_patch, (-1, mask_patch.shape[3], mask_patch.shape[4], mask_patch.shape[5])) # 675 x 64 x 64 x 32\n",
    "        \n",
    "        # processed_scan = np.stack((input_img,)*3, axis=-1) # Becomes 675 x 64 x 64 x 32 x 3\n",
    "        # processed_mask = np.expand_dims(input_mask, axis=3) # Becomes 675 x 64 x 64 x 32 x 1 \n",
    "\n",
    "        for i in range(input_mask.shape[0]): # Iterate through every small cubes\n",
    "            if np.sum(input_mask[i]) > 0: # Extract small cubes where mask present\n",
    "                if len(scan_storage) >= desired_size:\n",
    "                    break\n",
    "                print(f'Storing Patient {patient_idx} data, Cube Num: {i}')\n",
    "                scan_storage.append(input_img[i])\n",
    "                mask_storage.append(input_mask[i])\n",
    "                print(f'Current Training Data: {len(scan_storage)}')\n",
    "\n",
    "    # scan_storage = np.array(scan_storage)\n",
    "    # mask_storage = np.array(mask_storage)\n",
    "    \n",
    "    # processed_scan = np.stack((scan_storage,)*3, axis=-1)\n",
    "    # processed_mask = np.expand_dims(mask_storage, axis=4)\n",
    "    \n",
    "    return scan_storage, mask_storage, patients_processed\n",
    "\n",
    "def check_patients_processed(processed_patients_list):\n",
    "    x = Counter(processed_patients_list)\n",
    "    top = 5\n",
    "    if len(processed_patients_list) == len(set(processed_patients_list)):\n",
    "        print(f\"No duplicates of patients processed detected. {len(processed_patients_list)} patients processed\")\n",
    "    else:\n",
    "        print(f'Total patients processed: {len(processed_patients_list)}')\n",
    "        print(f\"Unique Patients processed: {len(set(processed_patients_list))}\")\n",
    "        print(f\"Top {top} occuring patients: \")\n",
    "        for i in range(top):\n",
    "            print(f\"Patient {x.most_common()[i][0]} : {x.most_common()[i][1]} times \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_mask, train_patients = process_one(train_path, train_mask_path, 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing 1 small cube for sense check\n",
    "@interact\n",
    "def explore_prediction(layer = (0,31), view = [\"axial\", \"sagittal\",\"coronal\"]):\n",
    "    if view == 'axial':\n",
    "        img_array_view = train_data[0][layer, :, :]\n",
    "        msk_array_view = train_mask[0][layer, :, :]\n",
    "    elif view == 'coronal':\n",
    "        img_array_view = train_data[0][:,layer,:]\n",
    "        msk_array_view = train_mask[0][:,layer,:]\n",
    "    else:\n",
    "        img_array_view = train_data[0][:,:,layer]\n",
    "        msk_array_view = train_mask[0][:,:,layer]\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_array_view, cmap='gray', aspect = \"auto\")\n",
    "    plt.title('Img', fontsize=10)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(msk_array_view, cmap='gray', aspect = \"auto\")\n",
    "    plt.title('Mask', fontsize=10)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data, valid_mask, valid_patients = process_one(valid_path, valid_mask_path, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)\n",
    "train_mask = np.array(train_mask)\n",
    "valid_data = np.array(valid_data)\n",
    "valid_mask = np.array(valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.stack((train_data,)*3, axis=-1)\n",
    "valid_data = np.stack((valid_data,)*3, axis=-1)\n",
    "\n",
    "train_mask = np.expand_dims(train_mask, axis=4)\n",
    "valid_mask = np.expand_dims(valid_mask, axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and coefficients to be used during training:\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    smoothing_factor = 1.0\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return (2. * K.sum(flat_y_true * flat_y_pred) + smoothing_factor) / (K.sum(flat_y_true) + K.sum(flat_y_pred) + smoothing_factor)\n",
    "\n",
    "def dice_coefficient_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    return 1.0 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "#Define parameters for our model.\n",
    "n_classes = 1\n",
    "patch_size = 32\n",
    "channels=3\n",
    "\n",
    "LR = 0.001\n",
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "\n",
    "\n",
    "model = build_unet((64,64,32,3), n_classes = 1)\n",
    "model.compile(optimizer = opt, loss=dice_coefficient_loss, metrics=dice_coefficient)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/home/tester/jianhoong/jh_fyp_work/3D_UNet/trials/3DUNet_ModelCSVLogs/UNet_Approach3_v5.csv'\n",
    "model_checkpoint_path = '/home/tester/jianhoong/jh_fyp_work/3D_UNet/ModelCheckpoints/Approach3_v5.hdf5'\n",
    "\n",
    "my_callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "    EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n",
    "    CSVLogger(csv_path, separator = ',', append = True),\n",
    "    ModelCheckpoint(filepath = model_checkpoint_path,\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    verbose = 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "history = model.fit(train_data, \n",
    "        train_mask,\n",
    "        batch_size=2, \n",
    "        epochs=50,\n",
    "        verbose=1,\n",
    "        validation_data=(valid_data, valid_mask),\n",
    "        callbacks = my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
