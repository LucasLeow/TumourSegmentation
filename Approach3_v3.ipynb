{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "\n",
    "import nrrd\n",
    "import random\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, Conv3DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Activation, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = Conv3D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)   #Not in the original network. \n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv3D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)  #Not in the original network\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPooling3D((2, 2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block\n",
    "#skip features gets input from encoder for concatenation\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv3DTranspose(num_filters, (2, 2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Build Unet using the blocks\n",
    "def build_unet(input_shape, n_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024) #Bridge\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    if n_classes == 1:  #Binary\n",
    "      activation = 'sigmoid'\n",
    "    else:\n",
    "      activation = 'softmax'\n",
    "\n",
    "    outputs = Conv3D(n_classes, 1, padding=\"same\", activation=activation)(d4)  #Change the activation based on n_classes\n",
    "    print(activation)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/'\n",
    "\n",
    "z_train = os.path.join(data_dir, 'training_data_z')\n",
    "z_train_image = os.path.join(z_train, 'training_images/training_images')\n",
    "z_train_mask = os.path.join(z_train, 'training_masks/training_masks')\n",
    "\n",
    "z_valid = os.path.join(data_dir, 'valid_data_z')\n",
    "z_valid_image = os.path.join(z_valid, 'valid_images/valid_images')\n",
    "z_valid_mask = os.path.join(z_valid, 'valid_masks/valid_masks')\n",
    "\n",
    "z_test = os.path.join(data_dir, 'testing_data_z')\n",
    "z_test_image = os.path.join(z_test, 'testing_images/testing_images')\n",
    "z_test_mask = os.path.join(z_test, 'testing_masks/testing_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nrrd_file(filepath):\n",
    "    '''read and load volume'''\n",
    "    pixelData, header = nrrd.read(filepath)\n",
    "    return pixelData[:,:,:96]\n",
    "\n",
    "def normalize(volume):\n",
    "    min = -1000 # min value of our data : -1000\n",
    "    max = 5000 # max value of our data : 5013\n",
    "    range = max - min\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / range\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "def process_scan(path):\n",
    "    volume = read_nrrd_file(path)\n",
    "    volume = normalize(volume)\n",
    "    # volume = resize_volume(volume)\n",
    "    return volume\n",
    "\n",
    "def sorted_alnum(l):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key : [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [4,5,9,10,15,16,18,19,20,21,22,23,28,29,33,34,35,37,44,51,52,58,64,68,69,72,73,76,77,79,82,83,90,91,92,93,94,95,97,103,104,116,123,126,129,131,134,136,139,142,149,151,155,157,159,162,163,164,169,170,171,179,180,186,188,191,192,198,200,201,206,207,208,209]\n",
    "valid_data = [213,214,215,217,219,220,221,224,226,227,228,231,237,238,240,243,248,249,250,254,255,256,257,264,266,270]\n",
    "test_data = [271,272,274,277,280,283,286,287,288,293,294,296,297,298,300,303,305,306,314,315,317,320,322,340,342,345,347]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = sorted_alnum([os.path.join(z_train_image, file) for file in os.listdir(z_train_image)  if int(re.findall(r'\\d+', file)[0]) in train_data])\n",
    "train_mask_path = sorted_alnum([os.path.join(z_train_mask, file) for file in os.listdir(z_train_mask)  if int(re.findall(r'\\d+', file)[0]) in train_data])\n",
    "\n",
    "valid_path = sorted_alnum([os.path.join(z_valid_image, file) for file in os.listdir(z_valid_image)  if int(re.findall(r'\\d+', file)[0]) in valid_data])\n",
    "valid_mask_path = sorted_alnum([os.path.join(z_valid_mask, file) for file in os.listdir(z_valid_mask)  if int(re.findall(r'\\d+', file)[0]) in valid_data])\n",
    "\n",
    "test_path = sorted_alnum([os.path.join(z_test_image, file) for file in os.listdir(z_test_image)  if int(re.findall(r'\\d+', file)[0]) in test_data])\n",
    "test_mask_path = sorted_alnum([os.path.join(z_test_mask, file) for file in os.listdir(z_test_mask)  if int(re.findall(r'\\d+', file)[0]) in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, list_IDs,labels,batch_size = 32, dim = (64,64,32), n_channels = 3, n_classes=1,shuffle=True):\n",
    "    self.dim = dim\n",
    "    self.batch = batch_size\n",
    "    self.labels = labels\n",
    "    self.list_IDs = list_IDs\n",
    "    self.n_channels = n_channels\n",
    "    self.shuffle = shuffle\n",
    "    self.on_epoch_end() # Triggered at start & end of epoch\n",
    "\n",
    "def on_epoch_end(self):\n",
    "    self.indexes = np.arange(len(self.list_IDs))\n",
    "    if self.shuffle == True: # Shuffling makes model more robust\n",
    "        np.random.shuffle(self.indexes)    \n",
    "        \n",
    "def __data_generation(self,list_IDs_temp):\n",
    "    X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "    y = np.empty((self.batch_size), dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(scan_paths, mask_paths,desired_size = 3000):\n",
    "    scan_storage = list()\n",
    "    mask_storage = list()\n",
    "    patients_processed = list()\n",
    "\n",
    "    while len(scan_storage) < desired_size:\n",
    "        if len(scan_storage) >= desired_size:\n",
    "            break\n",
    "        random_idx = random.randint(0, len(scan_paths)-1)\n",
    "        patient_idx = int(re.findall(r'\\d+', scan_paths[random_idx][-14:-11])[0])\n",
    "        patients_processed.append(patient_idx) # Extract numerical patient index from path string\n",
    "\n",
    "        print(f'Processing Patient {patient_idx} data')\n",
    "        scan_pixelData = process_scan(scan_paths[random_idx])\n",
    "        mask_pixelData = read_nrrd_file(mask_paths[random_idx])\n",
    "\n",
    "        scan_patch = patchify(scan_pixelData, (64,64,32), step = 32) # Yield 16 x 16 x 3 of size 32 x 32 x 32 cubes\n",
    "        mask_patch = patchify(mask_pixelData, (64,64,32), step = 32)\n",
    "\n",
    "        input_img = np.reshape(scan_patch, (-1, scan_patch.shape[3], scan_patch.shape[4], scan_patch.shape[5])) # Collapse 16 x 16 x 3 into 768 cubes x 32 x 32 x 32\n",
    "        input_mask = np.reshape(mask_patch, (-1, mask_patch.shape[3], mask_patch.shape[4], mask_patch.shape[5]))\n",
    "\n",
    "        for i in range(input_mask.shape[0]):\n",
    "            if np.sum(input_mask[i]) > 0:\n",
    "                print(f'Storing Patient {patient_idx} data, Cube Num: {i}')\n",
    "                scan_storage.append(input_img[i])\n",
    "                mask_storage.append(input_mask[i])\n",
    "                print(f'Current Training Data: {len(scan_storage)}')\n",
    "\n",
    "    scan_storage = np.array(scan_storage)\n",
    "    mask_storage = np.array(mask_storage)\n",
    "    \n",
    "    processed_scan = np.stack((scan_storage,)*3, axis=-1)\n",
    "    processed_mask = np.expand_dims(mask_storage, axis=4)\n",
    "    \n",
    "    return processed_scan, processed_mask, patients_processed\n",
    "\n",
    "def check_patients_processed(processed_patients_list):\n",
    "    x = Counter(processed_patients_list)\n",
    "    top = 5\n",
    "    if len(processed_patients_list) == len(set(processed_patients_list)):\n",
    "        print(f\"No duplicates of patients processed detected. {len(processed_patients_list)} patients processed\")\n",
    "    else:\n",
    "        print(f'Total patients processed: {len(processed_patients_list)}')\n",
    "        print(f\"Unique Patients processed: {len(set(processed_patients_list))}\")\n",
    "        print(f\"Top {top} occuring patients: \")\n",
    "        for i in range(top):\n",
    "            print(f\"Patient {x.most_common()[i][0]} : {x.most_common()[i][1]} times \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_mask, train_patients = process_one(train_path, train_mask_path, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data, valid_mask, valid_patients = process_one(valid_path, valid_mask_path, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and coefficients to be used during training:\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smoothing_factor = 1\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return (2. * K.sum(flat_y_true * flat_y_pred) + smoothing_factor) / (K.sum(flat_y_true) + K.sum(flat_y_pred) + smoothing_factor)\n",
    "\n",
    "def dice_coefficient_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "#Define parameters for our model.\n",
    "n_classes = 1\n",
    "patch_size = 32\n",
    "channels=3\n",
    "\n",
    "LR = 0.001\n",
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "\n",
    "\n",
    "model = build_unet((64,64,32,3), n_classes = 1)\n",
    "model.compile(optimizer = opt, loss=dice_coefficient_loss, metrics=dice_coefficient)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.input_shape)\n",
    "print(train_data.shape)\n",
    "print(model.output_shape)\n",
    "print(train_mask.shape)\n",
    "print(\"-------------------\")\n",
    "print(train_data.max())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/home/tester/jianhoong/jh_fyp_work/3D_UNet/trials/3DUNet_ModelCSVLogs/UNet_Approach3_v3.csv'\n",
    "model_checkpoint_path = '/home/tester/jianhoong/jh_fyp_work/3D_UNet/ModelCheckpoints/Approach3_v3.hdf5'\n",
    "\n",
    "my_callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "    CSVLogger(csv_path, separator = ',', append = True),\n",
    "    ModelCheckpoint(filepath = model_checkpoint_path,\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    verbose = 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "history = model.fit(train_data, \n",
    "        train_mask,\n",
    "        batch_size=2, \n",
    "        epochs=50,\n",
    "        verbose=1,\n",
    "        validation_data=(valid_data, valid_mask),\n",
    "        callbacks = my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation IoU and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['dice_coefficient']\n",
    "val_acc = history.history['val_dice_coefficient']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training Dice')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Dice')\n",
    "plt.title('Training and validation Dice')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_mask, test_patients = process_one(test_path, test_mask_path, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "from ipywidgets import interact, interactive, IntSlider, ToggleButtons\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(model_checkpoint_path, custom_objects = {'dice_coefficient_loss':dice_coefficient_loss,\n",
    "                                                                                   'dice_coefficient':dice_coefficient})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_number = random.randint(0, len(test_data))\n",
    "\n",
    "test_img = test_data[test_img_number]\n",
    "ground_truth = test_mask[test_img_number]\n",
    "\n",
    "test_img_input = np.expand_dims(test_img,0)\n",
    "test_pred = model.predict(test_img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_img.shape)\n",
    "print(ground_truth.shape)\n",
    "print(test_pred[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading immediately from .nrrd image instead of np array\n",
    "@interact\n",
    "def explore_prediction(layer = (0,31), view = [\"axial\", \"sagittal\",\"coronal\"]):\n",
    "    if view == 'axial':\n",
    "        img_array_view = test_img[layer, :, :]\n",
    "        msk_array_view = ground_truth[layer, :, :]\n",
    "        pred_array_view = test_pred[0][layer, :, :]\n",
    "    elif view == 'coronal':\n",
    "        img_array_view = test_img[:,layer,:]\n",
    "        msk_array_view = ground_truth[:,layer,:]\n",
    "        pred_array_view = test_pred[0][:,layer,:]\n",
    "    else:\n",
    "        img_array_view = test_img[:,:,layer]\n",
    "        msk_array_view = ground_truth[:,:,layer]\n",
    "        pred_array_view = test_pred[0][:,:,layer]\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img_array_view, cmap='gray', aspect = \"auto\")\n",
    "    plt.title('Img', fontsize=10)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(msk_array_view, cmap='gray', aspect = \"auto\")\n",
    "    plt.title('Mask', fontsize=10)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred_array_view, cmap='gray', aspect = \"auto\")\n",
    "    plt.title('Prediction', fontsize=10)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
