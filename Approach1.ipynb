{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nrrd\n",
    "import zipfile\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import SimpleITK as sitk\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify, unpatchify\n",
    "from ipywidgets import interact, interactive, IntSlider, ToggleButtons\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import segmentation_models_3D as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/'\n",
    "\n",
    "z_train = os.path.join(data_dir, 'training_data_z')\n",
    "z_train_image = os.path.join(z_train, 'training_images/training_images')\n",
    "z_train_mask = os.path.join(z_train, 'training_masks/training_masks')\n",
    "\n",
    "# z_valid = os.path.join(data_dir, 'valid_data_z')\n",
    "# z_valid_image = os.path.join(z_valid, 'valid_images/valid_images')\n",
    "# z_valid_mask = os.path.join(z_valid, 'valid_masks/valid_masks')\n",
    "\n",
    "# z_test = os.path.join(data_dir, 'testing_data_z')\n",
    "# z_test_image = os.path.join(z_test, 'testing_images/testing_images')\n",
    "# z_test_mask = os.path.join(z_test, 'testing_masks/testing_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nrrd_file(filepath):\n",
    "    '''read and load volume'''\n",
    "    pixelData, header = nrrd.read(filepath)\n",
    "    return pixelData\n",
    "\n",
    "def normalize(volume):\n",
    "    min = -1000 # min value of our data : -1000\n",
    "    max = 5000 # max value of our data : 5013\n",
    "    range = max - min\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / range\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "def resize_volume(img):\n",
    "    '''resizing across z-axis'''\n",
    "    desired_depth = 128\n",
    "    desired_width = 256\n",
    "    desired_height = 256\n",
    "\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "\n",
    "    depth_factor = 1 / (current_depth / desired_depth)\n",
    "    width_factor = 1 / (current_width / desired_width)\n",
    "    height_factor = 1/ (current_height / desired_height)\n",
    "    '''rotating image to fix orientation'''\n",
    "    img = ndimage.rotate(img, 90, reshape = False)\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order = 1)\n",
    "    return img\n",
    "\n",
    "def process_scan(path):\n",
    "    volume = read_nrrd_file(path)\n",
    "    volume = normalize(volume)\n",
    "    volume = resize_volume(volume)\n",
    "    return volume\n",
    "\n",
    "def sorted_alnum(l):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key : [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key = alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_5_dim = [1, 2, 7, 11, 17, 24, 25, 26, 27, 30, 32, 39, 40, 41, 42, 43, 45, 46, 50, 56, 57, 59, 61, 62, 63, 65, 66, 67, 70, 71, 74, 75, 78, 80, 84, 86, 98, 99, 100, 101, 102, 111, 113, 114, 115, 120, 121, 124, 125, 127, 128, 130, 133, 135, 137, 138, 141, 143, 146, 148, 150, 152, 153, 154, 158, 160, 161, 165, 166, 167, 168, 173, 174, 175, 176, 177, 178, 181, 182, 183, 187, 189, 194, 195, 196, 197, 203, 204]\n",
    "train_5_dim = [1]\n",
    "test_5_dim = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = sorted_alnum([os.path.join(z_train_image, file) for file in os.listdir(z_train_image)  if int(re.findall(r'\\d+', file)[0]) in train_5_dim])\n",
    "train_mask_path = sorted_alnum([os.path.join(z_train_mask, file) for file in os.listdir(z_train_mask)  if int(re.findall(r'\\d+', file)[0]) in train_5_dim])\n",
    "\n",
    "test_path = sorted_alnum([os.path.join(z_train_image, file) for file in os.listdir(z_train_image)  if int(re.findall(r'\\d+', file)[0]) in test_5_dim])\n",
    "test_mask_path = sorted_alnum([os.path.join(z_train_mask, file) for file in os.listdir(z_train_mask)  if int(re.findall(r'\\d+', file)[0]) in test_5_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scans = np.array([process_scan(path) for path in train_path])\n",
    "train_mask_scans = np.array([process_scan(path) for path in train_mask_path])\n",
    "\n",
    "test_scans = np.array([process_scan(path) for path in test_path])\n",
    "test_mask_scans = np.array([process_scan(path) for path in test_mask_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask_scans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up 3D UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'mobilenetv2'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scans = np.stack((train_scans,) * 3, axis = -1) # Stacking input img by itself , 3 times. To accomodate SM library requirements\n",
    "train_msk = np.expand_dims(train_mask_scans, axis = 4) # Mask requires 1 channel for SM library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scans = np.stack((train_scans,) * 3, axis = -1) \n",
    "train_msk = np.expand_dims(train_mask_scans, axis = 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scans = preprocess_input(train_scans)\n",
    "test_scans = preprocess_input(test_scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "\n",
    "dice_loss = sm.losses.DiceLoss()\n",
    "CE_loss = sm.losses.BinaryCELoss()\n",
    "total_loss = dice_loss + CE_loss \n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold = 0.5), sm.metrics.FScore(threshold = 0.5)]\n",
    "\n",
    "model = sm.Unet(\n",
    "    BACKBONE, \n",
    "    classes = 1,\n",
    "    input_shape = (256, 256, 128, 3),\n",
    "    encoder_weights = 'imagenet',\n",
    "    activation = 'sigmoid')\n",
    "\n",
    "model.compile(optimizer = opt, loss = total_loss, metrics = metrics)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_scans,\n",
    "    train_mask_scans,\n",
    "    batch_size = 1,\n",
    "    epochs = 25,\n",
    "    verbose = 1,\n",
    "    validation_data = (test_scans, test_mask_scans)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
