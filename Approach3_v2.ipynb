{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "\n",
    "import nrrd\n",
    "import random\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, Conv3DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Activation, MaxPool2D, Concatenate\n",
    "\n",
    "from ipywidgets import interact, interactive, IntSlider, ToggleButtons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.keras.__version__)\n",
    "print(tf.__version__)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices[3])\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[3], True)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[1], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/'\n",
    "\n",
    "z_train = os.path.join(data_dir, 'training_data_z')\n",
    "z_train_image = os.path.join(z_train, 'training_images/training_images')\n",
    "z_train_mask = os.path.join(z_train, 'training_masks/training_masks')\n",
    "\n",
    "z_valid = os.path.join(data_dir, 'valid_data_z')\n",
    "z_valid_image = os.path.join(z_valid, 'valid_images/valid_images')\n",
    "z_valid_mask = os.path.join(z_valid, 'valid_masks/valid_masks')\n",
    "\n",
    "z_test = os.path.join(data_dir, 'testing_data_z')\n",
    "z_test_image = os.path.join(z_test, 'testing_images/testing_images')\n",
    "z_test_mask = os.path.join(z_test, 'testing_masks/testing_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Helper Functions\n",
    "def read_nrrd_file(filepath):\n",
    "    '''read and load volume'''\n",
    "    pixelData, header = nrrd.read(filepath)\n",
    "    return pixelData[:,:,:96]\n",
    "\n",
    "def normalize(volume):\n",
    "    min = -1000 # min value of our data : -1000\n",
    "    max = 5000 # max value of our data : 5013\n",
    "    range = max - min\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / range\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "def process_scan(path):\n",
    "    volume = read_nrrd_file(path)\n",
    "    volume = normalize(volume)\n",
    "    return volume\n",
    "\n",
    "def sorted_alnum(l):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text \n",
    "    alphanum_key = lambda key : [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key = alphanum_key)\n",
    "\n",
    "def training_generator():\n",
    "    train_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/training_data_z/training_images/training_images'\n",
    "    train_mask_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/training_data_z/training_masks/training_masks'\n",
    "\n",
    "    train_data_with_96 = [15, 16, 20, 21, 23, 33, 37, 52, 64, 69, 72, 76, 77, 83, 92, 94, 95, 97, 103, 116, 123, 129, 157, 159, 162, 163, 170, 171, 180, 191, 192, 198, 200, 207, 209]\n",
    "    \n",
    "    train_path = sorted_alnum([os.path.join(train_dir, file) for file in os.listdir(train_dir)  if int(re.findall(r'\\d+', file)[0]) in train_data_with_96])\n",
    "    train_mask_path = sorted_alnum([os.path.join(train_mask_dir, file) for file in os.listdir(train_mask_dir)  if int(re.findall(r'\\d+', file)[0]) in train_data_with_96])\n",
    "    \n",
    "    for i in range(len(train_path)):\n",
    "        x = process_scan(train_path[i])\n",
    "        y = read_nrrd_file(train_mask_path[i])\n",
    "        \n",
    "        yield x,y\n",
    "        \n",
    "def validation_generator(valid_path, valid_mask):\n",
    "    \n",
    "    valid_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/valid_data_z/valid_images/valid_images'\n",
    "    valid_mask_dir = '/home/tester/jianhoong/jh_fyp_work/ct_scans_data/raw_data/valid_data_z/valid_masks/valid_masks'\n",
    "\n",
    "    valid_data_with_96 = [213, 219, 221, 227, 231, 238, 240, 243, 249, 250, 255, 256, 257, 270]\n",
    "\n",
    "    valid_path = sorted_alnum([os.path.join(valid_dir, file) for file in os.listdir(valid_dir)  if int(re.findall(r'\\d+', file)[0]) in valid_data_with_96])\n",
    "    valid_mask_path = sorted_alnum([os.path.join(valid_mask_dir, file) for file in os.listdir(valid_mask_dir)  if int(re.findall(r'\\d+', file)[0]) in valid_data_with_96])\n",
    "    \n",
    "    for i in range(len(valid_path)):\n",
    "        x = process_scan(valid_path[i])\n",
    "        y = read_nrrd_file(valid_mask_path[i])\n",
    "        yield x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = tf.data.Dataset.from_generator(training_generator,(tf.float32, tf.float32))\n",
    "validation_loader = tf.data.Dataset.from_generator(validation_generator,(tf.float32, tf.float32))\n",
    "\n",
    "ds = train_loader.batch(10)\n",
    "\n",
    "train_dataset = train_loader.shuffle(4)\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "train_dataset = train_dataset.batch(2, drop_remainder=True).prefetch(8)\n",
    "\n",
    "valid_dataset = validation_loader.shuffle(4)\n",
    "valid_dataset = valid_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "valid_dataset = valid_dataset.batch(2, drop_remainder=True).prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input, num_filters):\n",
    "    x = Conv3D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)   #Not in the original network. \n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv3D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)  #Not in the original network\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPooling3D((2, 2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block\n",
    "#skip features gets input from encoder for concatenation\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv3DTranspose(num_filters, (2, 2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Build Unet using the blocks\n",
    "def build_unet(input_shape, n_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024) #Bridge\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    if n_classes == 1:  #Binary\n",
    "      activation = 'sigmoid'\n",
    "    else:\n",
    "      activation = 'softmax'\n",
    "\n",
    "    outputs = Conv3D(n_classes, 1, padding=\"same\", activation=activation)(d4)  #Change the activation based on n_classes\n",
    "    print(activation)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and coefficients to be used during training:\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    smoothing_factor = 1.0\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return (2. * K.sum(flat_y_true * flat_y_pred) + smoothing_factor) / (K.sum(flat_y_true) + K.sum(flat_y_pred) + smoothing_factor)\n",
    "\n",
    "def dice_coefficient_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    return 1.0 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "#Define parameters for our model.\n",
    "n_classes = 1\n",
    "patch_size = 32\n",
    "channels=3\n",
    "\n",
    "LR = 0.001\n",
    "opt = tf.keras.optimizers.Nadam(LR)\n",
    "\n",
    "\n",
    "model = build_unet((64,64,32,3), n_classes = 1)\n",
    "model.compile(optimizer = opt, loss=dice_coefficient_loss, metrics=dice_coefficient)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/home/tester/jianhoong/jh_fyp_work/3D_UNet/trials/3DUNet_ModelCSVLogs/UNet_Approach3_v5.csv'\n",
    "model_checkpoint_path = '/home/tester/jianhoong/jh_fyp_work/3D_UNet/ModelCheckpoints/Approach3_v5.hdf5'\n",
    "\n",
    "my_callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n",
    "    EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n",
    "    CSVLogger(csv_path, separator = ',', append = True),\n",
    "    ModelCheckpoint(filepath = model_checkpoint_path,\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    verbose = 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = train_dataset.take(1)\n",
    "# images, labels = list(data)[0]\n",
    "# images = images.numpy()\n",
    "# image = images[0]\n",
    "# print(\"Dimension of the CT scan is:\", image.shape)\n",
    "# plt.imshow(np.squeeze(image[:, :, 30]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.input_shape)\n",
    "# print(train_data.shape)\n",
    "print(model.output_shape)\n",
    "# print(train_mask.shape)\n",
    "print(\"-------------------\")\n",
    "# print(train_data.max())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "history = model.fit(\n",
    "        train_dataset, \n",
    "        validation_data = valid_dataset,\n",
    "        epochs=50,\n",
    "        shuffle = True,\n",
    "        verbose=2,\n",
    "        callbacks = my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
